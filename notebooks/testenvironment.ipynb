{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install recipe-scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907997f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9821d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_scrapers import scrape_me\n",
    "from tests import ScraperTest\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121a292",
   "metadata": {},
   "source": [
    "For these websites I believe we'll need to add in a crawl delay of 1. So let's figure out how to do that before we start pulling all of the links and writing that daggum for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = scrape_me('https://www.allrecipes.com/recipe/25863/lobster-casserole/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13232175",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.title()\n",
    "scraper.total_time()\n",
    "scraper.yields()\n",
    "scraper.ingredients()\n",
    "scraper.instructions()\n",
    "scraper.image()\n",
    "scraper.host()\n",
    "scraper.links()\n",
    "scraper.nutrients() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c1147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scraper.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.yields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.total_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf23ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.yields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ing = scraper.ingredients()\n",
    "ing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ed79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618727e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.cuisine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784dfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.category()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46766802",
   "metadata": {},
   "outputs": [],
   "source": [
    "calories = scraper.nutrients()['calories']\n",
    "calories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81588645",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.nutrients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.allrecipes.com/search/results/?search=casserole'\n",
    "    \n",
    "response = requests.get(URL)\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9995b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response.text)\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f061b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a', 'href', class_='card__titleLink manual-link-behavior elementFont__titleLink margin-8-bottom').get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d08822",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe0 = soup.find_all(class_='card__detailsContainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes  = pd.DataFrame()\n",
    "titles = []\n",
    "links = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b3962",
   "metadata": {},
   "source": [
    "url = 'https://www.allrecipes.com/recipe/'\n",
    "#We are taking the hall of fame for loop and appending it for use here\n",
    "for page in range(45684,45694):\n",
    "    #Ok this is very confusing to me to have the range there, but the above link uses a similar method \n",
    "    scraper = scrape_me(url + str(page))\n",
    "    #soup = BS(pagedata.text)\n",
    "    titles.append(scraper.title())\n",
    "    #recipes['total_time'] = scraper.total_time()\n",
    "    #recipes['yields'] = scraper.yields()\n",
    "    #recipes['ingredients'] = scraper.ingredients()\n",
    "    #recipes['instructions'] = scraper.instructions()\n",
    "    #recipes['image'] = scraper.image()\n",
    "    #recipes['host'] = scraper.host()\n",
    "    print(scraper.title())\n",
    "    links.append(scraper.links())\n",
    "    #recipes['nutrients'] = scraper.nutrients() \n",
    "    time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d7c49",
   "metadata": {},
   "source": [
    "Ok it looks like we're getting some titles! Let's try a simpler way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "total_time = []\n",
    "yields = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scraper.title()\n",
    "scraper.total_time()\n",
    "scraper.yields()\n",
    "scraper.ingredients()\n",
    "scraper.instructions()\n",
    "scraper.image()\n",
    "scraper.host()\n",
    "scraper.links()\n",
    "scraper.nutrients() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys                       \n",
    "import time\n",
    "\n",
    "# Web scraper for infinite scrolling page \n",
    "driver = webdriver.Chrome(executable_path=r\"C:\\Users\\phili\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "driver.get(\"https://www.bonappetit.com/search?q=casserole\") #trying out bon appetit first?\n",
    "time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    # scroll one screen height each time\n",
    "    driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "    # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ec0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=r\"C:\\Users\\phili\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "driver.get(\"https://www.bonappetit.com/search?q=casserole\")\n",
    "urls = []\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "for parent in soup.find_all(class_=\"hed\"):\n",
    "    a_tag = parent.find(\"a\")\n",
    "    base = \"https://www.bonappetit.com/search?q=casserole\"\n",
    "    link = a_tag.attrs['href']\n",
    "    url = urljoin(base, link)\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e1bb3",
   "metadata": {},
   "source": [
    "Wow!!!! This works!! Exciting!! Now I just need to do this same thing for any other infinite scrolling website assuming the next part of this works.\n",
    "\n",
    "For reference, I'm using the good ol' https://medium.com/analytics-vidhya/using-python-and-selenium-to-scrape-infinite-scroll-web-pages-825d12c24ec7 . Shout out to my boy Kuan Wei for the dope tutorial.\n",
    "\n",
    "This bottom code RIGHT HERE. THIS is what's driving this project forward. This is the key. This is the future. We are truly doing this thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "##### Web scrapper for infinite scrolling page on bon appetit#####\n",
    "driver = webdriver.Chrome(executable_path=r\"C:\\Users\\phili\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "driver.get(\"https://www.bonappetit.com/search?q=casserole\")\n",
    "time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "scroll_pause_time = 2 # I'm setting this time to two because using 1 did not load bon appetit properly.\n",
    "screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    # scroll one screen height each time\n",
    "    driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "    # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break \n",
    "\n",
    "##### Extract Bon Appetit URLs #####\n",
    "urls = []\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "for parent in soup.find_all(class_=\"hed\"):\n",
    "    a_tag = parent.find(\"a\")\n",
    "    base = \"https://www.bonappetit.com/search?q=casserole\"\n",
    "    link = a_tag.attrs['href']\n",
    "    url = urljoin(base, link)\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac35c4e",
   "metadata": {},
   "source": [
    "Using a scroll_pause_time of 1 was not reloading the page, so I'm trying 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5dacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "total_time = []\n",
    "yields = []\n",
    "instructions = []\n",
    "ingredients = []\n",
    "images = []\n",
    "hosts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in urls:\n",
    "    #Ok this is very confusing to me to have the range there, but the above link uses a similar method \n",
    "    scraper = scrape_me(link)\n",
    "    #soup = BS(pagedata.text)\n",
    "    titles.append(scraper.title())\n",
    "    total_time.append(scraper.total_time())\n",
    "    yields.append(scraper.yields())\n",
    "    #recipes['ingredients'] = scraper.ingredients()\n",
    "    instructions.append(scraper.instructions())\n",
    "    images.append(scraper.image())\n",
    "    hosts.append(scraper.host())\n",
    "    print(scraper.title())\n",
    "    #links.append(scraper.links())\n",
    "    #recipes['nutrients'] = scraper.nutrients() \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d153eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270a157",
   "metadata": {},
   "source": [
    "Ok, I'm feeling pretty good about this method. It seems to get all of the instructions on here just fine, where then I can use Regex to pull out the information that I'm hoping to find in there. Like oven temperature, cooking methods, \"let stand\" included, \"garnish\" included, IS SALT INCLUDED??, etc. Lost of fun stuff to be added here!! Can't wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117359b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients=\"\".join(str(elem) for elem in scraper.ingredients())\n",
    "ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.ingredients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a16ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.nutrients() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082e02a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
